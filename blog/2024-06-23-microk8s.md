---
layout: post
title:  "HAProxy with TLS wildcard certifiacte"
date:   2024-06-23
image:  /assets/blog/2022-01-11_gauss_hills.png
categories: blog
tags:
  - k8s
---


## Idea

Spawn multiple web services from subdomains,
e.g. `https://a.domain.org` and `https://b.domain.org`,
having a proper Let's Encrypt TLS wildcard certifiacte 
following the always amazing Digial Ocean tutorials:
<https://www.digitalocean.com/community/tutorials/how-to-create-let-s-encrypt-wildcard-certificates-with-certbot>


## The remote server

A virtual private server (VPS) with low affordable resources:
- 2 cpus
- 2 GB memory
- 80 GB storage

## DNS setup

The provider of the remote server enables DNS management for a hosted domain.
After setting up a wildcard DNS entry as given in the Digial Ocean tutorial,
connectivity to arbitrary subdomains was checked via wget:
```
wget domain.org
--2024-06-23 11:18:49--  http://a.domain.org/
Resolving a.domain.org (a.domain.org)... 42.42.42.42

wget a.domain.org
--2024-06-23 11:18:49--  http://a.domain.org/
Resolving a.domain.org (a.domain.org)... 42.42.42.42

wget b.domain.org
--2024-06-23 11:18:49--  http://b.domain.org/
Resolving b.domain.org (b.domain.org)... 42.42.42.42
```
where `42.42.42.42` is a placeholder for the real public IP address used.


## Kubernetes (kubeadm) might be too expensive

> 2 GiB or more of RAM per machine--any less leaves little room for your apps.

<https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm>

## Installing MicroK8s

> MicroK8s runs in as little as 540MB of memory

<https://microk8s.io/docs/getting-started>

```
sudo snap install microk8s --classic --channel=1.30

sudo usermod -a -G microk8s $USER
mkdir -p ~/.kube
chmod 0700 ~/.kube
```


### Make the cluster availbe from the local machine.

<https://microk8s.io/docs/working-with-kubectl>

```
$ sudo lsof -i -P -n | grep LISTEN
...
kubelite  130549            root    7u  IPv6 566425      0t0  TCP *:16443 (LISTEN)
...
```
Checking this port shows that indeed the Kube API server is running on port `16443`.
```
curl -k https://localhost:16443
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
```
After the port was opened on the remote server's firewall,
it was possible to reach the Kube API server from the local machine.

The output of the command
```
microk8s config
```
has to be copied to the file `$HOME/.kube/config`.
Then one finally can log off the remote server and work from the local system.


### Final thoughts

MicroK8s comes already with the NGINX ingress controller availalbe
<https://microk8s.io/docs/addon-ingress>
but still I would like to give HAProxy a try.
```
microk8s enable ingress
```

## HAProxy

<https://www.haproxy.com/content-library/ebooks/haproxy-in-kubernetes-supercharge-your-ingress-routing>
<https://www.haproxy.com/documentation/kubernetes-ingress/community/installation/on-prem/>

```
helm repo add haproxytech \
    https://haproxytech.github.io/helm-charts
```




```
helm install haproxy-kubernetes-ingress haproxytech/kubernetes-ingress \
  --create-namespace              \
  --namespace haproxy-controller  \
  --version 1.40.0                \
  --set controller.kind=DaemonSet \
  --set controller.daemonset.useHostPort=true
```

In case to later remove the HAProxy ingress,
the following helm command can be run:
```
helm uninstall --namespace haproxy-controller haproxy-kubernetes-ingress
```

By default the HAProxy ingress does not mark itself as default one.
To avoid assigning to each ingress the desired ingress class,
one can edit the `haproxy` ingress class and add the following annotation
<https://kubernetes.io/docs/concepts/services-networking/ingress/#default-ingress-class>:
```
kubectl edit ingressclasses.networking.k8s.io haproxy
```
```yaml
metadata:
  annotation:
    ingressclass.kubernetes.io/is-default-class: "true"
```

Now it is time to try out the demo application from the HAProxy documentation.
As the documentation was a bit out of date,
a few modifications had to be made for the Ingress resource.
```
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.27.0
        ports:
        - containerPort: 80

---

apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: default
  labels:
    app: nginx
  annotations:
    haproxy.org/check: "enabled"
    haproxy.org/forwarded-for: "enabled"
    haproxy.org/load-balance: "roundrobin"
spec:
  selector:
    app: nginx
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80

---

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx
  namespace: default
spec:
  rules:
  - host: mround.org
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
EOF
```

### Some final thoughts and open questions

Trouble with detecting if the ingress-controller works

My favorite command to check if a Linux process is listening on a port
```
sudo lsof -i -P -n | grep LISTEN
```
does not work in this case.
I would be happy to find a better command to verify that HAProxy is indeed listening on port 80 and 443.
To my surprise,
with the HAProxy setup and working,
installing a Ubuntu native NGINX on top,
the command above reports that NGINX is listening on port 80,
but effectively all the traffic is routed to the Kubernetes ingress-controller.

I need to study more to fully understands what happens here.


## Getting the TLS wildcard

### Use Cert-Manager for each Ingress

<https://cert-manager.io/docs/tutorials/acme/nginx-ingress/>

### Configure HAProxy with a generated wildcard cert

